{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.回答以下理论问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 请写一下TF-IDF的计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf_{x,j}=\\frac{n_{x}}{N_{j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$idf_{x}=\\log{\\frac{N}{N_{x}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TF-IDF_{x,j}=tf_{x,j}*idf_{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x为单词，j为第j个文档，N为文档总数量，$N_{x}$为包含单词x的文档总数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA算法的基本假设是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设语料库中包含K个主题，每篇文档有一个文档-主题分布和主题-词分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 在TextRank算法中构建图的权重是如何得到的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果以词为节点，权重为边两端词的共现次数；如果以句子为节点，权重为边两端句子的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 什么是命名实体识别？ 有什么应用场景？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等，是关系抽取、事件抽取、知识图谱、信息提取、问答系统、句法分析、机器翻译等诸多NLP任务的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.NLP主要有哪几类任务 ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务，如文本分类、情感分类；生成任务，如机器翻译、问答系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 手动实现TextRank算法 (在新闻数据中随机提取100条新闻训练词向量和做做法测试）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 提示：\n",
    " 1. 确定窗口，建立图链接。   \n",
    " 2. 通过词向量相似度确定图上边的权重\n",
    " 3. 根据公式实现算法迭代(d=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  抽取关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.导入数据\n",
    "with open('sample.txt') as f1:\n",
    "    data = f1.read()\n",
    "data = data.replace('\\n', '')\n",
    "data = data.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f2:\n",
    "    stopwords = f2.read().split('\\n')\n",
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as psg\n",
    "\n",
    "def textrank2(document, niters = 10, span=5, topK=20, d=0.85, allowPOS=('ns', 'n', 'vn', 'v')):\n",
    "    after_cut = [x.word for x in jieba.posseg.cut(document) if x.flag in allowPOS and len(x.word) > 1 and x.word not in stopwords]\n",
    "    words = list(set(after_cut))\n",
    "    co_matrix = np.zeros((len(words), len(words))) + 1.0 / len(words)\n",
    "    ws = np.ones((len(words)))\n",
    "    \n",
    "#     共现矩阵\n",
    "    for i in range(len(after_cut)):\n",
    "        m = words.index(after_cut[i])\n",
    "            \n",
    "        for j in range(i + 1, i + span):\n",
    "            if j >= len(words):\n",
    "                break                \n",
    "            n = words.index(after_cut[j])\n",
    "            co_matrix[m, n] += 1\n",
    "                \n",
    "    for k in range(niters):\n",
    "#         if k == 0:\n",
    "        ws = (1 - d) + d * np.dot(ws, 1. / np.sum(co_matrix, axis = 1).reshape(-1, 1) * co_matrix)\n",
    "\n",
    "    min_size = min(ws)\n",
    "    max_size = max(ws)\n",
    "#     print(ws)\n",
    "    \n",
    "    results = [(words[i], (ws[i] - min_size) / (max_size - min_size)) for i in range(len(ws))]\n",
    "    results = sorted(results, key = lambda x : x[1], reverse = True)\n",
    "    \n",
    "    return results[:topK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/ng/1yxn2hp94vb65hy2rqh73__c0000gp/T/jieba.cache\n",
      "Loading model cost 0.927 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('病例', 1.0),\n",
       " ('疫情', 0.9973072422229192),\n",
       " ('复工', 0.9895934985149306),\n",
       " ('复学', 0.9226446017685198),\n",
       " ('感染者', 0.9151776553874014),\n",
       " ('风险', 0.8648847467766471),\n",
       " ('上海', 0.6335717605689725),\n",
       " ('聚集', 0.6325296461897005),\n",
       " ('没有', 0.6073076710962794),\n",
       " ('武汉', 0.5644594131707368),\n",
       " ('口罩', 0.48862754559207183),\n",
       " ('可能', 0.4869703046649232),\n",
       " ('中国', 0.482833591088373),\n",
       " ('控制', 0.4722476958769905),\n",
       " ('复产', 0.4688120729329363),\n",
       " ('核酸', 0.40147107693877826),\n",
       " ('需要', 0.39473945136597927),\n",
       " ('保持', 0.3878763785300145),\n",
       " ('传播', 0.3690985730014459),\n",
       " ('距离', 0.36707770892197167)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('疫情', 1.0),\n",
       " ('病例', 0.6057541971353849),\n",
       " ('没有', 0.6041099614918801),\n",
       " ('感染者', 0.5321139769217231),\n",
       " ('复工', 0.5146290559377263),\n",
       " ('风险', 0.5051330146452009),\n",
       " ('疫苗', 0.4887178555535026),\n",
       " ('新冠', 0.47717365987422805),\n",
       " ('聚集', 0.4672679646935086),\n",
       " ('复学', 0.4666314228793633),\n",
       " ('可能', 0.38000092540078684),\n",
       " ('群体', 0.3614637932326081),\n",
       " ('防控', 0.35666470662798394),\n",
       " ('需要', 0.3423462224818712),\n",
       " ('免疫', 0.33351942219242287),\n",
       " ('病毒', 0.3264158957330911),\n",
       " ('武汉', 0.2858046972767431),\n",
       " ('控制', 0.2804712528125678),\n",
       " ('吃饭', 0.2739091641044968),\n",
       " ('中国', 0.26842355750653746)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jieba.analyse import *\n",
    "textrank(data, withWeight = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.pagerank??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取关键句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(document):\n",
    "    sents = []\n",
    "    marks = '[。！；？]'\n",
    "    sentences = re.split(marks, document)[:-1]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = ''.join(re.findall(r'[\\u4e00-\\u9fa5]+', sentence))\n",
    "        sents.append([x for x in jieba.cut(sentence) if x not in stopwords])\n",
    "    \n",
    "    return sents, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_seg, sentences = cut(data)\n",
    "wordmodel = Word2Vec(sentences_seg, size = 100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordmodel2 = Word2Vec.load('../NLP_Project1/CC/model2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordmodel.wv[\"疫情\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank3(sents, sentences):\n",
    "    sentences_vector=[]\n",
    "    n = len(sents)\n",
    "    \n",
    "    # 生成平均句向量\n",
    "    for sen in sents:\n",
    "        sum_vec = np.zeros((100,))\n",
    "        for word in sen:\n",
    "            if word in wordmodel:\n",
    "                sum_vec += wordmodel.wv[word]\n",
    "            else:\n",
    "                print(word, \"not in wordmodel.\")\n",
    "\n",
    "        sentences_vector.append(sum_vec/(len(sen) + 1e-5))\n",
    "    \n",
    "#     print(sentences_vector)\n",
    "    \n",
    "    # 生成句向量的相似度矩阵\n",
    "    sim_matrix = np.ones((n, n)) + 1.0 / n\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                sim_matrix[i, j] = cosine_similarity(sentences_vector[i].reshape(1,-1), sentences_vector[j].reshape(1,-1))[0,0]\n",
    "\n",
    "    \n",
    "    # 手写\n",
    "    scores = np.ones(n)\n",
    "    for k in range(100):\n",
    "        scores = 0.15 + 0.85 * np.dot(scores, 1. / np.sum(sim_matrix, axis = 1).reshape(-1, 1) * sim_matrix)\n",
    "    \n",
    "    # 调用nx.pagerank()并排序\n",
    "#     nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "# #    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), key = lambda x:x[0], reverse=True)\n",
    "    \n",
    "    return ranked_sentences\n",
    "\n",
    "def print_abstract(ranked_sentences, topn = 5):\n",
    "    abstract = ''\n",
    "    num_sentences = min(topn, len(ranked_sentences))\n",
    "    for i in range(num_sentences):\n",
    "        abstract += ranked_sentences[i][1]\n",
    "        abstract += '。'\n",
    "    print(abstract)\n",
    "    \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张文宏回应外媒指责：如果“隐藏病例数”，武汉敢打开么。如果藏着掖着很多病例，武汉敢打开吗。中国病例数相对较少是因为有隐藏病例。国外有很多媒体说中国的病例数这么少，上海这么少，有没有把病人给藏着掖着。不过，天气热了，病例数是会下降的。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'张文宏回应外媒指责：如果“隐藏病例数”，武汉敢打开么。如果藏着掖着很多病例，武汉敢打开吗。中国病例数相对较少是因为有隐藏病例。国外有很多媒体说中国的病例数这么少，上海这么少，有没有把病人给藏着掖着。不过，天气热了，病例数是会下降的。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = textrank3(sentences_seg, sentences)\n",
    "print_abstract(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抽取句子经过100轮迭代之后仍未收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选做 1.  提取新闻语料里的对话。(使用以上提取小数据即可）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：    \n",
    "1.寻找预料里具有表示说的意思。    \n",
    "2.使用语法分析提取句子结构。    \n",
    "3.检测谓语是否有表示说的意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择2. ： 电影评论分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个电影评论分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（采用爬虫技术爬取相关网页上的电影评论数据，例如猫眼电影评论，豆瓣电影评论）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.把所获得数据分解为训练集，验证集和测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.选用相应算法构建模型，并测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择3：文章自动续写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个文章自动续写的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（根据你的兴趣采用爬虫技术爬去相关网站上的文本数据内容：比如故事网站，小说网站等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.选取模型，并训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.展示一些你模型的输出例子。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
