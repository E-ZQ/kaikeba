{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?    什么是自动编码器？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是1）数据相关的,2）有损的，3）从样本中自动学习的。\n",
    "\n",
    "搭建一个自动编码器需要完成下面三样工作：1）搭建编码器，2）搭建解码器，3）设定一个损失函数，用以衡量由于压缩而损失掉的信息。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化，例如SGD。\n",
    "\n",
    "目前自编码器的应用主要有两个方面，第一是数据去噪，第二是进行可视化而降维。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?    贪心搜索和集束搜索有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq模型相当于一个条件语言模型，本质上学习的是一个条件概率，即给定输入𝑥，学习概率分布𝑃(𝑦|𝑥)。\n",
    "\n",
    "贪心搜索：在decoder的每步都选择最可能的单词，最后得到句子的每一个单词都是每一步认为最合适的单词，但这样并不保证整个句子的概率是最大的。\n",
    "\n",
    "集束搜索：在每步选取最可能的𝑘个结果，再从最后的𝑘个结果中选取最合适的句子，𝑘称为beam size，若𝑘 = 1则为贪心搜索。k越大，得到更好结果的可能性更大，但计算消耗也越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?    注意力机制是为了解决什么问题而提出来的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络学习中，一般而言模型的参数越多则模型的表达能力越强，模型所存储的信息量也越大，但这会带来信息过载的问题。通过引入注意力机制，在众多的输入信息中聚焦于对当前任务更为关键的信息，降低对其他信息的关注度，甚至过滤掉无关信息，就可以解决信息过载问题，并提高任务处理的效率和准确性。\n",
    "\n",
    "未加入注意力机制的RNN Encoder-Decoder框架中，生成目标句子的每一个单词时，使用的语义表示向量c都是同一个，也就说生成每一个单词时，并没有产生[c1,c2,..,cT′]这样与每个输出的单词相对应的多个不同的语义表示。那么在预测某个词yt时，任何输入单词对于它的重要性都是一样的，也就是注意力分散了。\n",
    "\n",
    "加入注意力机制后的RNN Encoder-Decoder框架，语义向量表示是随输出yi的变化而变化的ci，而非不变的c。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?之前介绍的词嵌入方法有什么弊端？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无法解决一词多义问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough) ELMo模型的结构是怎样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMLo使用一个双向的LSTM，由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然。\n",
    "\n",
    "ELMo利用预训练好的双向语言模型，然后根据具体输入从该语言模型中可以得到上下文依赖的当前词表示（对于不同上下文的同一个词的表示是不一样的），再当成特征加入到具体的NLP有监督模型里。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?与RNN相比，Transformer有什么优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：(1) 时间片 𝑡 的计算依赖 𝑡−1 时刻的计算结果，这样限制了模型的并行能力；(2) 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象，LSTM依旧无能为力。\n",
    "\n",
    "Transformer的提出解决了上面两个问题：(1) 首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量；(2) 其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN与LN的区别在于，LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；而BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于batch的大小和输入sequence的深度，因此可以用于batchsize为1和RNN中对边长的输入sequence的normalize操作。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?为什么在Transformer中我们要使用位置编码？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RNN中，对句子的处理是一个个word按顺序输入的；但在Transformer中，输入句子的所有word是同时处理，没有考虑词的排序和位置信息。\n",
    "\n",
    "位置编码使得Transformer可以衡量与word位置有关的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?简要描述什么是自注意力模型和多头注意力模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自注意力模型中的自注意力机制则发生在输入序列内部，或者输出序列内部，可以抽取到同一个句子内间隔较远的单词之间的联系，比如句法特征。而在软注意力Encoder-Decoder模型中，注意力机制是发生在编码器和解码器之间。\n",
    "\n",
    "多头注意力模型中，Query，Key，Value首先进过一个线性变换，然后输入到放缩点积attention，每一次算一个头，且每次Q，K，V进行线性变换的参数W是不一样的。将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果，这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?GPT模型中的主要单元是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?简述在NLP任务中如何使用GPT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）对于分类任务，直接输入文本；\n",
    "\n",
    "2）对于文本蕴涵任务，将前提和假设用一个Delim分割向量拼接后输入；\n",
    "\n",
    "3）对于文本相似度任务，分别在不同方向上使用Delim拼接后输入；\n",
    "\n",
    "4）对于问答多选择的任务，将每个答案和上下文拼接后输入。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?BERT中的masked语言模型是指？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在与训练时, 将句子中的部分token用\"mask\"这个特殊的token进行替换, 即将部分单词遮掩住, 然后目标就是预测\"mask\"对应位置的单词.\n",
    "\n",
    "这种训练的好处是不需要人工标注的数据. 只需要通过合适的方法, 对现有语料中的句子进行随机的遮掩即可得到可以用来训练的语料. 训练好的模型, 就可以直接使用."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?  BERT的输入是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT以一串单词作为输入（使用了WordPiece embedding作为词向量，并加入了位置向量和句子切分向量。并在每一个文本输入前加入了一个CLS向量，后面会有这个向量作为具体的分类向量），这些单词不断地想编码器栈上层流动。每一层都要经过自注意力层和前馈网络，然后在将其交给下一个编码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks.简要描述如何在其它NLP任务中使用BERT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）句子关系类任务，输入部分加上一个起始和终结符号，句子之间加个分隔符；输出部分把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层。\n",
    "\n",
    "2）分类任务，输入部分增加起始和终结符号，输出部分和句子关系任务类似改造；\n",
    "\n",
    "3）序列标注任务，输入部分和单句分类是一样的，输出部分Transformer最后一层每个单词对应位置都进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT使用了left-to-right的Transformer，因为采用了传统语言模型所以更加适合用于自然语言生成类的任务 (NLG)。\n",
    "\n",
    "BERT使用了双向的Transformer架构，可以获得两边的单词，更适合用于自然语言理解任务 (NLU)。\n",
    "\n",
    "GPT-2基于transformer模型的decoder架构构建，本质是自回归。包括分类和翻译在内的许多问题都可以等价地表述为自回归问题，或者可以显著地从一个强大的预训练自回归语言模型中受益，因此自回归语言模型是具有很强的泛化性。\n",
    "\n",
    "GPT在fine-tuning时引入了SEP和CLS，而BERT是在预训练时引入的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras-cn.readthedocs.io/en/latest/legacy/blog/autoencoder/\n",
    "    \n",
    "https://www.cnblogs.com/Luv-GEM/p/10712256.html\n",
    "    \n",
    "https://www.cnblogs.com/mantch/p/11591937.html\n",
    "    \n",
    "https://www.cnblogs.com/robert-dlut/p/8638283.html\n",
    "    \n",
    "https://www.cnblogs.com/robert-dlut/p/9824346.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
